{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ea5a0b",
   "metadata": {},
   "source": [
    "## LSTM 구조\n",
    "\n",
    "<img src=\"https://github.com/LeeHyeJin91/PapersWithCode/assets/43728746/0cb19aef-8f61-41d3-a4ff-dc7cc8f67b75\" width=500 hight=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a282446",
   "metadata": {},
   "source": [
    "## LSTM 계산그래프\n",
    "\n",
    "<img src=\"https://github.com/LeeHyeJin91/PapersWithCode/assets/43728746/f3eb1195-8db7-443d-a616-41d887cd2436\" width=700 hight=500 />\n",
    "\n",
    "<img src=\"https://github.com/LeeHyeJin91/PapersWithCode/assets/43728746/4a0f20c0-a791-4c6d-8944-68e920e8180c\" width=700 hight=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff076786",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9fb586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e2af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer:\n",
    "    \n",
    "    def __init__(self, W):\n",
    "        self.params = [W] # (V, D)\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "     \n",
    "    def forward(self, x):\n",
    "        # input: x (N, 1)\n",
    "        \n",
    "        W, = self.params\n",
    "        self.idx = x\n",
    "        \n",
    "        return W[x] # (N, D)\n",
    "        \n",
    "    def backward(self, dx):\n",
    "        # input: dx (N, D)\n",
    "        \n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dx) # dW self.idx 행에 dx더함 -> self.grads도 같이 바뀜\n",
    "        \n",
    "        return None\n",
    "\n",
    "class Embedding:\n",
    "    \n",
    "    def __init__(self, W):\n",
    "        \n",
    "        self.params = [W] # (V, D)\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.W = W\n",
    "        self.layers = []\n",
    "        \n",
    "    def forward(self, input_x):\n",
    "        # input: input_x  (N, T)\n",
    "        # output: x       (N, T, D)\n",
    "        \n",
    "        N, T = input_x.shape\n",
    "        V, D = self.W.shape\n",
    "        \n",
    "        x = np.empty((N, T, D), dtype='f')\n",
    "        for t in range(T):\n",
    "            layer = EmbeddingLayer(self.W)\n",
    "            x[:, t, :] = layer.forward(input_x[:, t]) # (N, D)\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def backward(self, dx):\n",
    "        # input dx: (N, T, D)\n",
    "        \n",
    "        N, T, D = dx.shape\n",
    "        \n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dx[:, t, :])\n",
    "            grad += layer.grads[0]    # (V, D)\n",
    "        \n",
    "        self.grads[0][...] = grad\n",
    "         \n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe1af73",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59b760d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer:\n",
    "    \n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        \n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx),np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        \n",
    "        Wx, Wh, b = self.params # (D, 4H), (H, 4H) (4H, )\n",
    "        N, H = h_prev.shape\n",
    "        A = np.matmul(x, Wx) + np.matmul(h_prev, Wh) + b # (N, 4H)\n",
    "        \n",
    "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        f = sigmoid(A[:, :H])\n",
    "        g = np.tanh(A[:, H:2*H])\n",
    "        i = sigmoid(A[:, 2*H:3*H])\n",
    "        o = sigmoid(A[:, 3*H:])\n",
    "        \n",
    "        c_next = f * c_prev + i * g\n",
    "        h_next = o * np.tanh(c_next)\n",
    "        \n",
    "        self.cache = (x, h_prev, c_prev, f, g, i, o, c_next)\n",
    "        \n",
    "        return h_next, c_next\n",
    "        \n",
    "    def backward(self, dh_next, dc_next):\n",
    "        \n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, f, g, i, o, c_next = self.cache\n",
    "        \n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next**2)\n",
    "        \n",
    "        dc_prev = ds * f\n",
    "        df = ds * c_prev * f * (1 - f)\n",
    "        dg = ds * i * (1 - g**2)\n",
    "        di = ds * g * i * (1 - i)\n",
    "        do = dh_next * tanh_c_next * o * (1-o)\n",
    "        dA = np.hstack([df, dg, di, do]) # (N, 4H)\n",
    "        \n",
    "        dWh = np.matmul(h_prev.T, dA)    # (H, N) (N, 4H) \n",
    "        dh_prev = np.matmul(dA, Wh.T)    # (N, 4H) (4H, H)\n",
    "              \n",
    "        dWx = np.matmul(x.T, dA)         # (D, N) (N, 4H)\n",
    "        dx = np.matmul(dA, Wx.T)         # (N, 4H) (4H, D)\n",
    "        db = np.sum(dA, axis=0)\n",
    "        \n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebed4311",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        \n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        \n",
    "        self.stateful = stateful\n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.layers = []\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        # input: xs  (N, T, D)\n",
    "        # output: hs (N, T, H)\n",
    "        \n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H = Wh.shape[0]\n",
    "        \n",
    "        if self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        if self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "        \n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "        for t in range(T):\n",
    "            layer = LSTMLayer(Wx, Wh, b)\n",
    "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
    "            hs[:, t, :] = self.h\n",
    "            \n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        return hs\n",
    "        \n",
    "    def backward(self,dhs):\n",
    "        # input: dhs  (N, T, H)\n",
    "        # output: dxs (N, T, D)\n",
    "        \n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = Wx.shape[0]\n",
    "    \n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "        grads = [0, 0, 0]\n",
    "    \n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh, dc = layer.backward(dhs[:, t, :]+dh, dc)\n",
    "            dxs[:, t, :] = dx\n",
    "            \n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "        \n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        \n",
    "        self.dh = dh\n",
    "        \n",
    "        return dxs\n",
    "        \n",
    "    def set_state(self, h, c):\n",
    "        self.h = h\n",
    "        self.c = c\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "        self.c = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564a87c7",
   "metadata": {},
   "source": [
    "### Affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa54c4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    \n",
    "    def __init__(self, Wa, ba):\n",
    "        \n",
    "        self.params = [Wa, ba] # (H, V), (V, )\n",
    "        self.grads = [np.zeros_like(Wa), np.zeros_like(ba)]\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, h):\n",
    "        # input: h  (N, T, H)\n",
    "        # output: a (N, T, V)\n",
    "        \n",
    "        Wa, ba = self.params\n",
    "        N, T, H = h.shape\n",
    "        \n",
    "        h = h.reshape(N*T, -1)     # (NT, H)\n",
    "        a = np.matmul(h, Wa) + ba  # (NT, V)\n",
    "        a = a.reshape(N,T, -1)     # (N, T, V)\n",
    "        \n",
    "        self.cache = h\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        # input: da  (N, T, V)\n",
    "        # output: dh (N, T, H)\n",
    "        \n",
    "        Wa, ba = self.params\n",
    "        N, T, V = da.shape\n",
    "        h = self.cache             # (NT, H)    \n",
    "        da = da.reshape(N*T, -1)   # (NT, V)\n",
    "        \n",
    "        dWa = np.matmul(h.T, da)   # (H, NT) (NT, V)\n",
    "        dba = np.sum(da, axis=0)   # (V, )\n",
    "        dh = np.matmul(da, Wa.T)   # (NT, V) (V, H) \n",
    "        dh = dh.reshape(N, T, -1)  # (N, T, H)\n",
    "    \n",
    "        self.grads[0][...] = dWa\n",
    "        self.grads[1][...] = dba\n",
    "    \n",
    "        return dh\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d729b",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c526f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, a, label):\n",
    "        # input: a(N, T, V) label(N, T)\n",
    "        # output: y(N, T)\n",
    "        \n",
    "        N, T, V = a.shape\n",
    "        a = a.reshape(N*T, -1)                           # (NT, V)\n",
    "        label = label.reshape(N*T)                       # (NT, )\n",
    "        \n",
    "        # softmax 계산\n",
    "        a = a - a.max(axis=1, keepdims=True)\n",
    "        a_exp = np.exp(a)\n",
    "        a_stm = a_exp / a_exp.sum(axis=1, keepdims=True) # (NT, V)\n",
    "        \n",
    "        # 정답 label만 선택 \n",
    "        y = a_stm[np.arange(N*T), label]                 # (NT, )\n",
    "        self.cache = (y, label, a_stm)\n",
    "        \n",
    "        return y.reshape(N, T)\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        # input: dy  (N, T)\n",
    "        # output: da (N, T, V)\n",
    "        \n",
    "        N, T = dy.shape\n",
    "        dy = dy.reshape(N*T)                              # (NT, )\n",
    "        y, label, a_stm = self.cache                      # (NT, )\n",
    "        \n",
    "        a_stm[np.arange(N*T), label] = dy * (y * (1 - y)) # (NT, V)\n",
    "        a_stm = a_stm/(N*T)\n",
    "        da = a_stm.reshape(N, T, -1)                      # (N, T, V)\n",
    "\n",
    "        return da\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc06c96",
   "metadata": {},
   "source": [
    "### Cross Entrophy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdee6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEloss:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, y):\n",
    "        # input: y     (N, T)\n",
    "        # output: loss (1, 1)\n",
    "        \n",
    "        N, T = y.shape\n",
    "        _y = y.reshape(N*T) # (NT, )\n",
    "        \n",
    "        loss = -np.sum(np.log(_y))\n",
    "        loss = loss/(N*T)\n",
    "        self.cache = y\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dloss=1):\n",
    "        # input: dloss  \n",
    "        # output: dy (N, T)\n",
    "\n",
    "        y = self.cache      \n",
    "        N, T = y.shape\n",
    "        \n",
    "        y = y.reshape(N*T)   # (NT, )\n",
    "        dy = dloss * (-1/y) \n",
    "        \n",
    "        return dy.reshape(N, T)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a3d78c",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3345633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLM:\n",
    "    \n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        \n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        # 파라미터 초기화\n",
    "        W = (rn(V, D) / 100).astype('f')\n",
    "        Wx = (rn(D, 4*H) / np.sqrt(D)).astype('f')\n",
    "        Wh = (rn(H, 4*H) / np.sqrt(H)).astype('f')\n",
    "        b  = np.zeros(4*H).astype('f')\n",
    "        Wa = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        ba = np.zeros(V).astype('f')\n",
    "        \n",
    "        # 계층생성\n",
    "        self.layers = [Embedding(W),\n",
    "                       LSTM(Wx, Wh, b, stateful=True),\n",
    "                       Affine(Wa, ba)\n",
    "                      ]\n",
    "        self.softmax = Softmax()\n",
    "        self.loss = CEloss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        y = self.softmax.forward(x, label)\n",
    "        loss = self.loss.forward(y)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dloss=1):\n",
    "        \n",
    "        dout = self.loss.backward(dloss)\n",
    "        dout = self.softmax.backward(dout)\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()\n",
    "        \n",
    "    def save_params(self, file_name='lstm.pkl'):\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(self.params, f)\n",
    "            \n",
    "    def load_params(self, file_name='lstm.pkl'):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            self.params = pickle.load(f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1943ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33f7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
