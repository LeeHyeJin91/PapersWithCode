{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e552dee6",
   "metadata": {},
   "source": [
    "### Seq2Seq 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eaf052",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/LeeHyeJin91/PapersWithCode/assets/43728746/e94352e8-fe3d-45d3-a147-99e279ad5d29\" width=900 height=800 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad65606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4d4021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer:\n",
    "    \n",
    "    def __init__(self, W):\n",
    "        self.params = [W] # (V, D)\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "     \n",
    "    def forward(self, x):\n",
    "        # input: x (N, 1)\n",
    "        \n",
    "        W, = self.params\n",
    "        self.idx = x\n",
    "        \n",
    "        return W[x] # (N, D)\n",
    "        \n",
    "    def backward(self, dx):\n",
    "        # input: dx (N, D)\n",
    "        \n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dx) # dW self.idx 행에 dx더함 -> self.grads도 같이 바뀜\n",
    "        \n",
    "        return None\n",
    "\n",
    "class Embedding:\n",
    "    \n",
    "    def __init__(self, W):\n",
    "        \n",
    "        self.params = [W] # (V, D)\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.W = W\n",
    "        self.layers = []\n",
    "        \n",
    "    def forward(self, input_x):\n",
    "        # input: input_x  (N, T)\n",
    "        # output: x       (N, T, D)\n",
    "        \n",
    "        N, T = input_x.shape\n",
    "        V, D = self.W.shape\n",
    "        \n",
    "        x = np.empty((N, T, D), dtype='f')\n",
    "        for t in range(T):\n",
    "            layer = EmbeddingLayer(self.W)\n",
    "            x[:, t, :] = layer.forward(input_x[:, t]) # (N, D)\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def backward(self, dx):\n",
    "        # input dx: (N, T, D)\n",
    "        \n",
    "        N, T, D = dx.shape\n",
    "        \n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dx[:, t, :])\n",
    "            grad += layer.grads[0]    # (V, D)\n",
    "        \n",
    "        self.grads[0][...] = grad\n",
    "         \n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0c06872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer:\n",
    "    \n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        \n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx),np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        \n",
    "        Wx, Wh, b = self.params # (D, 4H), (H, 4H) (4H, )\n",
    "        N, H = h_prev.shape\n",
    "        A = np.matmul(x, Wx) + np.matmul(h_prev, Wh) + b # (N, 4H)\n",
    "        \n",
    "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        f = sigmoid(A[:, :H])\n",
    "        g = np.tanh(A[:, H:2*H])\n",
    "        i = sigmoid(A[:, 2*H:3*H])\n",
    "        o = sigmoid(A[:, 3*H:])\n",
    "        \n",
    "        c_next = f * c_prev + i * g\n",
    "        h_next = o * np.tanh(c_next)\n",
    "        \n",
    "        self.cache = (x, h_prev, c_prev, f, g, i, o, c_next)\n",
    "        \n",
    "        return h_next, c_next\n",
    "        \n",
    "    def backward(self, dh_next, dc_next):\n",
    "        \n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, f, g, i, o, c_next = self.cache\n",
    "        \n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next**2)\n",
    "        \n",
    "        dc_prev = ds * f\n",
    "        df = ds * c_prev * f * (1 - f)\n",
    "        dg = ds * i * (1 - g**2)\n",
    "        di = ds * g * i * (1 - i)\n",
    "        do = dh_next * tanh_c_next * o * (1-o)\n",
    "        dA = np.hstack([df, dg, di, do]) # (N, 4H)\n",
    "        \n",
    "        dWx = np.matmul(x.T, dA)         # (D, N) (N, 4H)\n",
    "        dx = np.matmul(dA, Wx.T)         # (N, 4H) (4H, D)\n",
    "        \n",
    "        dWh = np.matmul(h_prev.T, dA)    # (H, N) (N, 4H) \n",
    "        dh_prev = np.matmul(dA, Wh.T)    # (N, 4H) (4H, H)\n",
    "            \n",
    "        db = np.sum(dA, axis=0)\n",
    "        \n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "        \n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        \n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        \n",
    "        self.stateful = stateful\n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.layers = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input: x  (N, T, D)\n",
    "        # output: h (N, T, H)\n",
    "        \n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = x.shape\n",
    "        H = Wh.shape[0]\n",
    "        \n",
    "        if self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        if self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "        \n",
    "        h = np.empty((N, T, H), dtype='f')\n",
    "        for t in range(T):\n",
    "            layer = LSTMLayer(Wx, Wh, b)\n",
    "            self.h, self.c = layer.forward(x[:, t, :], self.h, self.c)\n",
    "            h[:, t, :] = self.h\n",
    "            \n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        return h\n",
    "        \n",
    "    def backward(self, dh):\n",
    "        # input: dh  (N, T, H)\n",
    "        # output: dx (N, T, D)\n",
    "        \n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dh.shape\n",
    "        D = Wx.shape[0]\n",
    "    \n",
    "        dx = np.empty((N, T, D), dtype='f')\n",
    "        _dh, dc = 0, 0\n",
    "        grads = [0, 0, 0]\n",
    "    \n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, _dh, dc = layer.backward(dh[:, t, :]+ _dh, dc)\n",
    "            dx[:, t, :] = dx\n",
    "            \n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "        \n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        \n",
    "        self.dh = _dh\n",
    "        \n",
    "        return dx\n",
    "        \n",
    "    def set_state(self, h, c=None):\n",
    "        self.h = h\n",
    "        self.c = c\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "        self.c = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d96db946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    \n",
    "    def __init__(self, Wa, ba):\n",
    "        \n",
    "        self.params = [Wa, ba] # (H, V), (V, )\n",
    "        self.grads = [np.zeros_like(Wa), np.zeros_like(ba)]\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, h):\n",
    "        # input: h  (N, T, H)\n",
    "        # output: a (N, T, V)\n",
    "        \n",
    "        Wa, ba = self.params\n",
    "        N, T, H = h.shape\n",
    "        \n",
    "        h = h.reshape(N*T, -1)     # (NT, H)\n",
    "        a = np.matmul(h, Wa) + ba  # (NT, V)\n",
    "        a = a.reshape(N, T, -1)    # (N, T, V)\n",
    "        \n",
    "        self.cache = h\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        # input: da  (N, T, V)\n",
    "        # output: dh (N, T, H)\n",
    "        \n",
    "        Wa, ba = self.params\n",
    "        N, T, V = da.shape\n",
    "        h = self.cache             # (NT, H)    \n",
    "        da = da.reshape(N*T, -1)   # (NT, V)\n",
    "        \n",
    "        dWa = np.matmul(h.T, da)   # (H, NT) (NT, V)\n",
    "        dba = np.sum(da, axis=0)   # (V, )\n",
    "        dh = np.matmul(da, Wa.T)   # (NT, V) (V, H) \n",
    "        dh = dh.reshape(N, T, -1)  # (N, T, H)\n",
    "    \n",
    "        self.grads[0][...] = dWa\n",
    "        self.grads[1][...] = dba\n",
    "    \n",
    "        return dh\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42fbb414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, a, label):\n",
    "        # input: a(N, T, V) label(N, T)\n",
    "        # output: y(N, T)\n",
    "        \n",
    "        N, T, V = a.shape\n",
    "        a = a.reshape(N*T, -1)                           # (NT, V)\n",
    "        label = label.reshape(N*T)                       # (NT, )\n",
    "        \n",
    "        # softmax 계산\n",
    "        a = a - a.max(axis=1, keepdims=True)\n",
    "        a_exp = np.exp(a)\n",
    "        a_stm = a_exp / a_exp.sum(axis=1, keepdims=True) # (NT, V)\n",
    "        \n",
    "        # 정답 label만 선택 \n",
    "        y = a_stm[np.arange(N*T), label]                 # (NT, )\n",
    "        self.cache = (y, label, a_stm)\n",
    "        \n",
    "        return y.reshape(N, T)\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        # input: dy  (N, T)\n",
    "        # output: da (N, T, V)\n",
    "        \n",
    "        N, T = dy.shape\n",
    "        dy = dy.reshape(N*T)                              # (NT, )\n",
    "        y, label, a_stm = self.cache                      # (NT, )\n",
    "        \n",
    "        a_stm[np.arange(N*T), label] = dy * (y * (1 - y)) # (NT, V)\n",
    "        a_stm = a_stm/(N*T)\n",
    "        da = a_stm.reshape(N, T, -1)                      # (N, T, V)\n",
    "\n",
    "        return da\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9263f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEloss:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, y):\n",
    "        # input: y     (N, T)\n",
    "        # output: loss (1, 1)\n",
    "        \n",
    "        N, T = y.shape\n",
    "        _y = y.reshape(N*T) # (NT, )\n",
    "        \n",
    "        loss = -np.sum(np.log(_y))\n",
    "        loss = loss/(N*T)\n",
    "        self.cache = y\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dloss=1):\n",
    "        # input: dloss  \n",
    "        # output: dy (N, T)\n",
    "\n",
    "        y = self.cache      \n",
    "        N, T = y.shape\n",
    "        \n",
    "        y = y.reshape(N*T)   # (NT, )\n",
    "        dy = dloss * (-1/y) \n",
    "        \n",
    "        return dy.reshape(N, T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01e8e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    \n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        \n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.rand\n",
    "        \n",
    "        W = (rn(V, D)/100).astype('f')\n",
    "        \n",
    "        Wx = (rn(D, 4*H) / np.sqrt(D)).astype('f') \n",
    "        Wh = (rn(H, 4*H) / np.sqrt(H)).astype('f') \n",
    "        b = np.zeros(4*H).astype('f')\n",
    "        \n",
    "        self.embd = Embedding(W)\n",
    "        self.lstm = LSTM(Wx, Wh, b, stateful=False)\n",
    "        \n",
    "        self.params = self.embd.params + self.lstm.params\n",
    "        self.grads = self.embd.grads + self.lstm.grads\n",
    "        \n",
    "        self.hs = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input:  x(N, T)\n",
    "        # output: h(N, H)\n",
    "        \n",
    "        xs = self.embd.forward(x)  # (N, T, D)\n",
    "        hs = self.lstm.forward(xs) # (N, T, H)\n",
    "        h = hs[:, -1, :]           # (N, H)\n",
    "        self.hs = hs\n",
    "        \n",
    "        return h\n",
    "        \n",
    "    def backward(self, dh):\n",
    "        # input: dh (N, H)\n",
    "        \n",
    "        dhs = np.zeros_like(self.hs)    # (N, T, H)\n",
    "        dhs[:, -1, :] = dh \n",
    "        \n",
    "        dout = self.lstm.backward(dhs)  # (N, T, D)\n",
    "        dout = self.embd.backward(dout) # embedding 업데이트 \n",
    "        \n",
    "        return dout\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37346cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    \n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        \n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.rand\n",
    "        \n",
    "        W = (rn(V, D)/100).astype('f')\n",
    "        \n",
    "        Wx = (rn(D, 4*H) / np.sqrt(D)).astype('f') \n",
    "        Wh = (rn(H, 4*H) / np.sqrt(H)).astype('f') \n",
    "        b = np.zeros(4*H).astype('f')\n",
    "        \n",
    "        Wa = (rn(H, V) / np.sqrt(H)).astype('f') \n",
    "        ba = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.embd = Embedding(W)\n",
    "        self.lstm = LSTM(Wx, Wh, b, stateful=True)\n",
    "        self.affine = Affine(Wa, ba)\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        \n",
    "        for layer in (self.embd, self.lstm, self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        # for training...\n",
    "        # input: x(N, T) h(N, H)\n",
    "        # output: a(N, T, V)\n",
    "        \n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        x = self.embd.forward(x)   # (N, T, D)\n",
    "        h = self.lstm.forward(x)   # (N, T, H)\n",
    "        a = self.affine.forward(h) # (N, T, V)\n",
    "        \n",
    "        return a \n",
    "        \n",
    "    def backward(self, da):\n",
    "        # input: da  (N, T, V)\n",
    "        # output: dh (N, H)\n",
    "        \n",
    "        dout = self.affine.backward(da)   # (N, T, H)\n",
    "        dout = self.lstm.backward(dout)   # (N, T, D)\n",
    "        dout = self.embd.backward(dout)   # 임베딩 업데이트\n",
    "        dh = self.lstm.dh                 # (N, H)\n",
    "        \n",
    "        return dh\n",
    "        \n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        # for inference...\n",
    "        # input: x \n",
    "        \n",
    "        self.lstm.set_state(h)\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        \n",
    "        for _ in range(sample_size):\n",
    "            \n",
    "            x = np.array(sample_id).reshape(1, 1) # (1, 1)\n",
    "            x = self.embd.forward(x)              # (1, 1, D)\n",
    "            h = self.lstm.forward(x)              # (1, 1, H)\n",
    "            a = self.affine.forward(a)            # (1, 1, V)\n",
    "            \n",
    "            sample_id = np.argmax(a.flatten())\n",
    "            sampled.append(sample_id)\n",
    "        \n",
    "        return sampled\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78156fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq:\n",
    "    \n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        \n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = Decoder(V, D, H)\n",
    "        self.softmax = Softmax()\n",
    "        self.loss = CEloss()\n",
    "        \n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # input: x(N, T) t(N, T)\n",
    "        # output: loss\n",
    "        \n",
    "        decoder_x = t[:, :-1]\n",
    "        decoder_t = t[:, 1:]\n",
    "        \n",
    "        h = self.encoder.forward(x)                # (N, H)\n",
    "        score = self.decoder.forward(decoder_x, h) # (N, T, V)\n",
    "        y = self.softmax(score, decoder_t)         # (N, T, 1)\n",
    "        loss = self.loss(y)\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dloss):\n",
    "        \n",
    "        dout = self.loss.backward(dloss)   # (N, T)\n",
    "        dout = self.softmax.backward(dout) # (N, T, V)\n",
    "        dh = self.decoder.backward(dout)   # (N, H)\n",
    "        dout = self.encoder.backward(dh)   # (N, T, D)\n",
    "        \n",
    "        return dout\n",
    "    \n",
    "    def generate(self, x, start_id, sample_size):\n",
    "        # input: x(N, T)\n",
    "        \n",
    "        h = self.encoder.forward(x)\n",
    "        sampled = self.generate(h, start_id, sample_size)\n",
    "        \n",
    "        return sampled\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbdf2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
