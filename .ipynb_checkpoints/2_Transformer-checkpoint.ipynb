{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e65069",
   "metadata": {},
   "source": [
    "### data  로드\n",
    "* https://github.com/cpm0722/transformer_pytorch/ 참고 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7665d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import spacy\n",
    "import os\n",
    "import pickle\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ce5d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en\n",
    "# ! python -m spacy download de\n",
    "\n",
    "spacy_lang_dict = {\n",
    "        'en': \"en_core_web_sm\",\n",
    "        'de': \"de_core_news_sm\"\n",
    "        }\n",
    "\n",
    "tokenizer_src = get_tokenizer(\"spacy\", spacy_lang_dict['en'])\n",
    "tokenizer_tgt = get_tokenizer(\"spacy\", spacy_lang_dict['de'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dfa0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = '../data/multi30k'\n",
    "raw_dir = os.path.join(cache_dir, \"raw\")\n",
    "\n",
    "train_file = os.path.join(cache_dir, \"train.pkl\")\n",
    "valid_file = os.path.join(cache_dir, \"valid.pkl\")\n",
    "test_file = os.path.join(cache_dir, \"test.pkl\")\n",
    "\n",
    "# train.pkl\n",
    "with open(os.path.join(raw_dir, \"train.en\"), \"r\") as f:\n",
    "    train_en = [text.rstrip() for text in f]\n",
    "    \n",
    "with open(os.path.join(raw_dir, \"train.de\"), \"r\") as f:\n",
    "    train_de = [text.rstrip() for text in f]\n",
    "    \n",
    "train = [(en, de) for en, de in zip(train_en, train_de)]\n",
    "\n",
    "# with open(train_file, \"wb\") as f:\n",
    "#         pickle.dump(train, f)\n",
    "        \n",
    "# valid.pkl\n",
    "\n",
    "with open(os.path.join(raw_dir, \"val.en\"), \"r\") as f:\n",
    "        valid_en = [text.rstrip() for text in f]\n",
    "with open(os.path.join(raw_dir, \"val.de\"), \"r\") as f:\n",
    "        valid_de = [text.rstrip() for text in f]\n",
    "valid = [(en, de) for en, de in zip(valid_en, valid_de)]\n",
    "\n",
    "# with open(valid_file, \"wb\") as f:\n",
    "#         pickle.dump(valid, f)\n",
    "        \n",
    "# test.pkl\n",
    "\n",
    "with open(os.path.join(raw_dir, \"test_2016_flickr.en\"), \"r\") as f:\n",
    "                test_en = [text.rstrip() for text in f]\n",
    "with open(os.path.join(raw_dir, \"test_2016_flickr.de\"), \"r\") as f:\n",
    "    test_de = [text.rstrip() for text in f]\n",
    "test = [(en, de) for en, de in zip(test_en, test_de)]\n",
    "\n",
    "# with open(test_file, \"wb\") as f:\n",
    "#         pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c83ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_vocab()\n",
    "\n",
    "def yield_tokens(is_src=True):\n",
    "    for text_pair in train:\n",
    "        if is_src:\n",
    "            yield [str(token) for token in tokenizer_src(text_pair[0])]\n",
    "        else:\n",
    "            yield [str(token) for token in tokenizer_tgt(text_pair[1])]\n",
    "\n",
    "            \n",
    "specials={\n",
    "        \"<unk>\": 0,\n",
    "        \"<pad>\": 1,\n",
    "        \"<sos>\": 2,\n",
    "        \"<eos>\": 3\n",
    "        }\n",
    "\n",
    "vocab_src_file = os.path.join(cache_dir, \"vocab_en.pkl\")\n",
    "vocab_src = build_vocab_from_iterator(yield_tokens(is_src=True), min_freq=2, specials=specials.keys())\n",
    "vocab_src.set_default_index(0)\n",
    "# with open(vocab_src_file, \"wb\") as f:\n",
    "#         pickle.dump(vocab_src, f)\n",
    "        \n",
    "vocab_tgt_file = os.path.join(cache_dir, \"vocab_de.pkl\")\n",
    "vocab_tgt = build_vocab_from_iterator(yield_tokens(is_src=False), min_freq=2, specials=specials.keys())\n",
    "vocab_tgt.set_default_index(0)\n",
    "# with open(vocab_tgt_file, \"wb\") as f:\n",
    "#         pickle.dump(vocab_tgt_file, f)\n",
    "        \n",
    "def get_transform(vocab):\n",
    "    max_seq_len = 256\n",
    "    sos_idx = 2\n",
    "    eos_idx = 3\n",
    "    pad_idx = 1\n",
    "    return T.Sequential(\n",
    "            T.VocabTransform(vocab),\n",
    "            T.Truncate(max_seq_len-2),\n",
    "            T.AddToken(token=sos_idx, begin=True),\n",
    "            T.AddToken(token=eos_idx, begin=False),\n",
    "            T.ToTensor(padding_value=pad_idx))\n",
    "\n",
    "transform_src = get_transform(vocab_src)\n",
    "transform_tgt = get_transform(vocab_tgt)\n",
    "\n",
    "\n",
    "def collate_fn(pairs):\n",
    "    src = [tokenizer_src(pair[0]) for pair in pairs]\n",
    "    tgt = [tokenizer_tgt(pair[1]) for pair in pairs]\n",
    "    batch_src = transform_src(src)\n",
    "    batch_tgt = transform_tgt(tgt)\n",
    "    return (batch_src, batch_tgt)\n",
    "\n",
    "train_iter = DataLoader(train, collate_fn=collate_fn)\n",
    "valid_iter = DataLoader(valid, collate_fn=collate_fn)\n",
    "test_iter = DataLoader(test, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f06da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (src, trg) in enumerate(train_iter):\n",
    "    trg_x = trg[:, :-1]\n",
    "    trg_y = trg[:, 1:]\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46147c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbf6c751",
   "metadata": {},
   "source": [
    "### mask 생성 \n",
    "* https://github.com/cpm0722/transformer_pytorch/ 참고 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38fe98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer class 내부 mask 생성방법 이해 \n",
    "\n",
    "def make_src_mask(src):\n",
    "    pad_mask = make_pad_mask(src, src)\n",
    "    return pad_mask\n",
    "\n",
    "def make_tgt_mask(tgt):\n",
    "    pad_mask = make_pad_mask(tgt, tgt)\n",
    "    seq_mask = make_subsequent_mask(tgt, tgt)\n",
    "    mask = pad_mask & seq_mask\n",
    "    return mask\n",
    "\n",
    "def make_src_tgt_mask(src, tgt):\n",
    "    pad_mask = make_pad_mask(tgt, src)\n",
    "    return pad_mask\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def make_pad_mask(query, key, pad_idx=1):\n",
    "    # query: (n_batch, query_seq_len)\n",
    "    # key: (n_batch, key_seq_len)\n",
    "    query_seq_len, key_seq_len = query.size(1), key.size(1)\n",
    "\n",
    "    key_mask = key.ne(pad_idx).unsqueeze(1).unsqueeze(2)  # (n_batch, 1, 1, key_seq_len)\n",
    "    key_mask = key_mask.repeat(1, 1, query_seq_len, 1)    # (n_batch, 1, query_seq_len, key_seq_len)\n",
    "\n",
    "    query_mask = query.ne(pad_idx).unsqueeze(1).unsqueeze(3)  # (n_batch, 1, query_seq_len, 1)\n",
    "    query_mask = query_mask.repeat(1, 1, 1, key_seq_len)      # (n_batch, 1, query_seq_len, key_seq_len)\n",
    "\n",
    "    mask = key_mask & query_mask\n",
    "    mask.requires_grad = False\n",
    "    return mask\n",
    " \n",
    "def make_subsequent_mask(query, key): #  make_no_peak_mask()\n",
    "    query_seq_len, key_seq_len = query.size(1), key.size(1)\n",
    "\n",
    "    tril = np.tril(np.ones((query_seq_len, key_seq_len)), k=0).astype('uint8') # lower triangle without diagonal\n",
    "    mask = torch.tensor(tril, dtype=torch.bool, requires_grad=False, device=query.device)\n",
    "    return mask\n",
    "\n",
    "## src mask \n",
    "query = src\n",
    "key = src\n",
    "pad_idx = 1\n",
    "\n",
    "query_seq_len, key_seq_len = query.size(1),key.size(1) # (n_batch, seq_len) 13\n",
    " \n",
    "key_mask = key.ne(pad_idx).unsqueeze(1).unsqueeze(2)  # (n_batch, 1, 1, key_seq_len)\n",
    "key_mask = key_mask.repeat(1, 1, query_seq_len, 1)    # (n_batch, 1, query_seq_len, key_seq_len)\n",
    "\n",
    "query_mask = query.ne(pad_idx).unsqueeze(1).unsqueeze(3) # (n_batch, 1, query_seq_len, 1)\n",
    "query_mask = query_mask.repeat(1, 1, 1, key_seq_len)     # (n_batch, 1, query_seq_len, key_seq_len)\n",
    "\n",
    "src_mask = key_mask & query_mask\n",
    "src_mask.requires_grad = False\n",
    "\n",
    "## src_tgt mask\n",
    "query = trg_x\n",
    "key = src\n",
    "pad_idx = 1\n",
    "\n",
    "query_seq_len, key_seq_len = query.size(1),key.size(1) # 14, 13\n",
    " \n",
    "key_mask = key.ne(pad_idx).unsqueeze(1).unsqueeze(2)  # (n_batch, 1, 1, key_seq_len)\n",
    "key_mask = key_mask.repeat(1, 1, query_seq_len, 1)    # (n_batch, 1, query_seq_len, key_seq_len)\n",
    "\n",
    "query_mask = query.ne(pad_idx).unsqueeze(1).unsqueeze(3) # (n_batch, 1, query_seq_len, 1)\n",
    "query_mask = query_mask.repeat(1, 1, 1, key_seq_len)     # (n_batch, 1, query_seq_len, key_seq_len)\n",
    "\n",
    "src_trg_mask = key_mask & query_mask\n",
    "src_trg_mask.requires_grad = False\n",
    "\n",
    "## trg mask \n",
    "pad_mask = make_pad_mask(trg_x, trg_x)\n",
    "\n",
    "# subsequent mask \n",
    "query = trg_x\n",
    "key = trg_x\n",
    "query_seq_len, key_seq_len = query.size(1), key.size(1) # 14, 14\n",
    "tril = np.tril(np.ones((query_seq_len, key_seq_len)), k=0).astype('uint8') # lower triangle without diagonal\n",
    "seq_mask = torch.tensor(tril, dtype=torch.bool, requires_grad=False, device=query.device)\n",
    "\n",
    "trg_mask = pad_mask & seq_mask\n",
    "trg_mask.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338976e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301674e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45906754",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "* https://github.com/hyunwoongko/transformer 참고 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e62c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1) # nn.Embedding 상속 \n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_embed = d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = [[1, 4, 204, 2, 5], [] ,...  []] (batch_size, seq_len)\n",
    "        out = self.embedding(x) * math.sqrt(self.d_embed) #(batch_size, seq_len, d_model)\n",
    "        return out\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니므로 단어의 위치 정보를 다른 방식으로 알려줘야 함\n",
    "    \n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        super(PostionalEncoding, self).__init__()\n",
    "        \n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device) # (max_len, d_model)\n",
    "        self.encoding.requires_grad = False # gradient 계산 필요없음 \n",
    "         \n",
    "        pos = torch.arange(0, max_len)     # if max_len= 50 then [0,1,..., 49]\n",
    "        pos = pos.float().unsqueeze(dim=1) # (max_len, 1)\n",
    "        \n",
    "        _2i = torch.arange(0, d_model, step=2).float() # [0,2,4, ..., 510]\n",
    "    \n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model))) # [0, 2, ...] \n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model))) # [1, 3, ...]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()    # [batch_size = 128, seq_len = 30]\n",
    "        return self.encoding[:seq_len, :] # [seq_len = 30, d_model = 512]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "896735e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    token embedding + positional encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        \n",
    "        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
    "        self.pos_emb = PostionalEncoding(d_model, max_len, device)\n",
    "        self.drop_out = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb(x)\n",
    "        return self.drop_out(tok_emb + pos_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13ea5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        '''\n",
    "        q (batch_size, num_head, seq_len, d_model//num_head)\n",
    "        k (batch_size, num_head, seq_len, d_model//num_head)\n",
    "        v (batch_size, num_head, seq_len, d_model//num_head)\n",
    "        '''\n",
    "        batch_size, num_head, seq_len, d_tensor = k.size()\n",
    "            \n",
    "        # 1. 유사도계산\n",
    "        k_t = k.transepose(2, 3)\n",
    "        score = q @ k_t  # (batch_size, num_head, seq_len, seq_len)\n",
    "\n",
    "        # 2. 마스크적용\n",
    "        if mask:\n",
    "            score = score.masked_fill(0, -100000)\n",
    "\n",
    "        # 3. 가중치 계산\n",
    "        score = score/math.sqrt(d_tensor)\n",
    "        score = self.softmax(score) # (batch_size, num_head, seq_len, seq_len)\n",
    "\n",
    "        # 4. 가중합 계산 \n",
    "        v = score @ v  # (batch_size, num_head, seq_len, d_model//num_head)\n",
    "        \n",
    "        return v, score # attention_value, attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "487653d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.num_head = num_head\n",
    "        self.attention = ScaleDotProductAttention()\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask):\n",
    "        \n",
    "        ## 1. weight matrix 계산\n",
    "        # _q (batch_size, seq_len, d_model)\n",
    "        # _k (batch_size, seq_len, d_model)\n",
    "        # _v (batch_size, seq_len, d_model)\n",
    "        _q, _k, _v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "        \n",
    "        ## 2. num_head 만큼 split\n",
    "        # q (batch_size, num_head, seq_len, d_model//num_head)\n",
    "        # k (batch_size, num_head, seq_len, d_model//num_head)\n",
    "        # v (batch_size, num_head, seq_len, d_model//num_head)\n",
    "        q, k, v = self.split(_q), self.split(_k), self.split(_v)\n",
    "        \n",
    "        ## 3. attention layer \n",
    "        out, attention_score = self.attention(q, k, v, mask)\n",
    "        \n",
    "        ## 4. num_head 합치기 \n",
    "        out = self.concat(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def split(self, tensor):\n",
    "        # tensor (batch_size, seq_len, d_model)\n",
    "        # spt_tensor (batch_size, num_head, seq_len, d_model//num_head)\n",
    "        batch_size, seq_len, d_model = tensor.size()\n",
    "        d_tensor = d_model // self.num_head\n",
    "        spt_tensor = tensor.view(batch_size, seq_len, self.num_head, d_tensor).transpose(1, 2)\n",
    "        \n",
    "        return spt_tensor\n",
    "        \n",
    "    def concat(self, tensor):\n",
    "        # tensor (batch_size, num_head, seq_len, d_model//num_head)\n",
    "        # concat_tensor (batch_size, seq_len, d_model)\n",
    "        batch_size, num_head, seq_len, d_tensor = tensor.size()\n",
    "        d_model = num_head * d_tensor\n",
    "        concat_tensor = tensor.transepose(1,2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        \n",
    "        return concat_tensor\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, eps=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model)) # (d_model)\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model)) # (d_model)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        mean = x.mean(dim=2, keepdim=True)                 # (batch_size, seq_len, 1), dim 부분을 없앰 \n",
    "        var = x.var(dim=-1, unbiased=False, keepdim=True)  # (batch_size, seq_len, 1)  dim=-1 마지막 차원\n",
    "\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)      # (batch_size, seq_len, d_model)\n",
    "        out = out * self.gamma + self.beta                 # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        return out \n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        \n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden, bias=True) # bias=True 디폴트 \n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x (batch_size, seq_len, d_model)\n",
    "        x = self.linear1(x)  # (batch_size, seq_len, hidden)\n",
    "        x = self.relu(x)     # (batch_size, seq_len, hidden)\n",
    "        x = self.dropout(x)  # (batch_size, seq_len, hidden)\n",
    "        x = self.linear2(x)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, n_head, hidden, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d_model, n_head)\n",
    "        \n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        self.ffn = PositionwiseFeedForward(d_model, hidden, drop_prob)\n",
    "        \n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        \n",
    "        # 1. compute self-attention\n",
    "        _x = x                                           # (batch_size, seq_len, d_model)\n",
    "        x = self.attention(q=x, k=x, v=x, mask=src_mask) # (batch_size, seq_len, d_model)\n",
    "            \n",
    "        # 2. add and norm\n",
    "        x = self.dropout1(x)                             # (batch_size, seq_len, d_model)\n",
    "        x = self.norm1(x + _x)                           # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # 3. positionwise feed forward network\n",
    "        _x = x                                           # (batch_size, seq_len, d_model)\n",
    "        x = self.ffn(x)                                  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # 4. add and norm\n",
    "        x = self.dropout2(x)                             # (batch_size, seq_len, d_model)\n",
    "        x = self.norm2(x + _x)                           # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device, n_head, ffn_hidden, num_layer):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embd = TransformerEmbedding(vocab_size, d_model, max_len, drop_prob, device)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_head, ffn_hidden, drop_prob) for _ in range(num_layer)]) \n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        \n",
    "        x = self.embd(x)\n",
    "        for encoder_layer in self.layers:\n",
    "            x = encoder_layer(x, src_mask)\n",
    "        return x\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, n_head, drop_prob, ffn_hidden):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, num_head=n_head)\n",
    "        \n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, num_head=n_head)\n",
    "        \n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        self.ffn = PositionwiseFeedForward(d_model, ffn_hidden, drop_prob)\n",
    "        \n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(p = drop_prob)\n",
    "        \n",
    "    def forward(self, dec, enc, trg_mask, src_trg_mask):\n",
    "        \n",
    "        # 1. compute self attention\n",
    "        _x = dec\n",
    "        x = self.self_attention(q=dec, k=dec, v=dec, mask=trg_mask) # 룩어헤드 마스크 \n",
    "    \n",
    "        # 2. add and norm \n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "        \n",
    "        # 3. compute encoder-decoder attention \n",
    "        _x = x\n",
    "        x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=src_trg_mask)  \n",
    "        \n",
    "        # 4. add and norm \n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + _x)\n",
    "        \n",
    "        # 5. pointwise feedforward \n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        # 6. add and norm \n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x + _x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, dec_voc_size, d_model, max_len, drop_prob, device, num_layer, n_head):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.emb = TransformerEmbedding(dec_voc_size, d_model, max_len, drop_prob, device)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_head, drop_prob, ffn_hidden) for _ in range(num_layer)])\n",
    "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
    "\n",
    "        \n",
    "    def forward(self, dec, enc, trg_mask, src_trg_mask):\n",
    "        x = self.emb(dec)\n",
    "        \n",
    "        for decoder_layer in self.layers :\n",
    "            x = decoder_layer(x, enc, trg_mask, src_trg_mask)\n",
    "\n",
    "        output = self.linear(x)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a76b5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, \\\n",
    "                 enc_voc_size, dec_voc_size, d_model, n_head, max_len, ffn_hidden, num_layer, drop_prob, device):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.trg_sos_idx = trg_sos_idx\n",
    "        self.device = device\n",
    "        \n",
    "        self.encoder = Encoder(enc_voc_size, d_model, max_len, drop_prob, device, n_head, ffn_hidden, num_layer)\n",
    "        self.decoder = Decoder(dec_voc_size, d_model, max_len, drop_prob, device, num_layer, n_head)\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        # 1. 마스크 생성\n",
    "        src_mask = self.make_pad_mask(src, src, self.src_pad_idx, self.src_pad.idx)\n",
    "        \n",
    "        trg_mask = self.make_pad_mask(trg, trg, self.trg_pad_idx, self.trg_pad_idx) * \\\n",
    "                   self.make_no_peak_mask(trg, trg)\n",
    "        \n",
    "        src_trg_mask = self.make_pad_mask(trg, src, self.trg_pad_idx, self.src_pad.idx)\n",
    "        # 2.인코더 \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        # 3. 디코더 \n",
    "        output = self.decoder(trg, enc_src, trg_mask, src_trg_mask)\n",
    "        \n",
    "        return output \n",
    "    \n",
    "    def make_pad_mask(self, q, k, q_pad_idx, k_pad_idx):\n",
    "        \n",
    "        q_seq_len, k_seq_len = q.size(1), k.size(1)        # (n_batch, seq_len) \n",
    "        \n",
    "        k_mask = k.ne(k_pad_idx).unsqueeze(1).unsqueeze(2) # (n_batch, 1, 1, key_seq_len)\n",
    "        k_mask = k_mask.repeat(1, 1, q_seq_len, 1)         # (n_batch, 1, query_seq_len, key_seq_len)\n",
    "                                                           # 0방향으로 1번, 1방향으로 1번, 2방향으로 len_q번 3방향으로 1번 반복하란 소리 \n",
    "\n",
    "        q_mask = q.ne(q_pad_idx).unsqueeze(1).unsqueeze(3) # (n_batch, 1, query_seq_len, 1)\n",
    "        q_mask = q_mask.repeat(1, 1, 1, k_seq_len)         # (n_batch, 1, query_seq_len, key_seq_len)\n",
    "\n",
    "        mask = k_mask & q_mask\n",
    "        return mask\n",
    "    \n",
    "\n",
    "    def make_no_peak_mask(self, q, k):\n",
    "        q_seq_len, k_seq_len = q.size(1), k.size(1)\n",
    "\n",
    "        # q_seq_len x k_seq_len\n",
    "        mask = torch.tril(torch.ones(q_seq_len, k_seq_len)).type(torch.BoolTensor).to(self.device)\n",
    "\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "58242d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,   19,   25,   15, 1169,  808,   17,   57,   84,  336, 1339,    5,\n",
      "            3]]) \n",
      "\n",
      "tensor([[   2,   21,   85,  257,   31,   87,   22,   94,    7,   16,  112, 7910,\n",
      "         3209,    4]])\n"
     ]
    }
   ],
   "source": [
    "# 인풋 데이터 이해  \n",
    "for idx, (src, trg) in enumerate(train_iter):\n",
    "    trg_x = trg[:, :-1]\n",
    "    trg_y = trg[:, 1:]\n",
    "    break\n",
    "    \n",
    "print(src, '\\n')\n",
    "print(trg_x)\n",
    "\n",
    "# model 선언 \n",
    "model = Transformer()\n",
    "output = model(srg, trg_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39767b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aed3502c",
   "metadata": {},
   "source": [
    "### train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bada77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/hyunwoongko/transformer/blob/master/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "185e0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 모델 선언 \n",
    "src_pad_idx = 1\n",
    "trg_pad_idx = 1\n",
    "trg_sos_idx = 0\n",
    "d_model = 64\n",
    "enc_voc_size = len(vocab_src)\n",
    "dec_voc_size = len(vocab_tgt)\n",
    "max_len = 1000\n",
    "ffn_hidden = 32\n",
    "n_heads = 2\n",
    "n_layers = 2\n",
    "drop_prob = 0.1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Transformer(src_pad_idx=src_pad_idx,\n",
    "                    trg_pad_idx=trg_pad_idx,\n",
    "                    trg_sos_idx=trg_sos_idx,\n",
    "                    d_model=d_model,\n",
    "                    enc_voc_size=enc_voc_size,\n",
    "                    dec_voc_size=dec_voc_size,\n",
    "                    max_len=max_len,\n",
    "                    ffn_hidden=ffn_hidden,\n",
    "                    n_head=n_heads,\n",
    "                    num_layer=n_layers,\n",
    "                    drop_prob=drop_prob,\n",
    "                    device=device).to(device)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# 가중치 초기화 \n",
    "model.apply(initialize_weights)\n",
    "\n",
    "# 이거랑 같은 코드  \n",
    "# for submodule in model.children() :\n",
    "#     initialize_weights(submodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419585dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) loss 정의\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f1027730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) optimizer 및 lr 스케쥴러 정의 \n",
    "from torch import nn, optim\n",
    "from torch.optim import Adam\n",
    "\n",
    "init_lr = 1e-5\n",
    "weight_decay = 5e-4\n",
    "adam_eps = 5e-9\n",
    "\n",
    "clip = 1.0\n",
    "inf = float('inf')\n",
    "\n",
    "optimizer = Adam(params=model.parameters(),\n",
    "                 lr=init_lr,\n",
    "                 weight_decay=weight_decay,\n",
    "                 eps=adam_eps)\n",
    "\n",
    "# 학습과정에서 learning rate를 조정하는 스케쥴러\n",
    "# ReduceLROnPlateau: 성능 향상이 없을 때 learning rate를 감소(성능기준: valid_loss나 metric 등으로 지정)\n",
    "factor = 0.9  # 감소시킬 비율 lr*factor \n",
    "patience = 10 # metric이 향상되지 않을 때, patience 만큼 참고, 그 이후에 lr 감소  \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 verbose=True,\n",
    "                                                 factor=factor,\n",
    "                                                 patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ea3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "num_epoch = 1000\n",
    "warmup = 100\n",
    "best_loss = inf\n",
    "\n",
    "for epoch in range(num_epoch)\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(train_iter):\n",
    "\n",
    "        # input \n",
    "        src = batch[0]\n",
    "        trg = batch[1]\n",
    "\n",
    "        # output\n",
    "        output = model(src, trg)\n",
    "\n",
    "        # loss\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        optimizer.zero_grad() # loss.backward()를 호출할때 초기설정은 매번 gradient를 더해주는 것으로 설정 따라서 한번 돌고 초기화해줘야함\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss = epoch_loss/(i+1)\n",
    "    print(f'epoch:{epoch}  loss:{epoch_loss}')\n",
    "    \n",
    "    \n",
    "    # 성능체크(scheduler 사용)\n",
    "    if epoch > warmup:\n",
    "        scheduler.step(valid_loss) # 이 경우 valid_loss를 성능으로 지정\n",
    "        \n",
    "    # 모델저장(valid_loss가 감소할때만 저장)\n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved/model-{0}.pt'.format(valid_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d5d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f464b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03a63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b4487b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b8416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
