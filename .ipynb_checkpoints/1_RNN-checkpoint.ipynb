{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef62c745",
   "metadata": {},
   "source": [
    "## RNN 구조\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/LeeHyeJin91/PapersWithCode/assets/43728746/b5eb542e-deef-4449-a6ec-dfd158f65c6a\" width=\"500\" height=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18844d",
   "metadata": {},
   "source": [
    "## RNN 계산그래프\n",
    "\n",
    "<img src=\"https://github.com/LeeHyeJin91/PapersWithCode/assets/43728746/151a0cbf-7087-4562-88c0-5e837537c265\" width=\"900\" height=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d997e6",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc65821",
   "metadata": {},
   "source": [
    "#### Embedding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0c58d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38416e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer:\n",
    "    \n",
    "    def __init__(self, W):\n",
    "        self.params = [W] # (V, D)\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "     \n",
    "    def forward(self, x):\n",
    "        # input: x (N, 1)\n",
    "        \n",
    "        W, = self.params\n",
    "        self.idx = x\n",
    "        \n",
    "        return W[x] # (N, D)\n",
    "        \n",
    "    def backward(self, dx):\n",
    "        # input: dx (N, D)\n",
    "        \n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dx) # dW self.idx 행에 dx더함 -> self.grads도 같이 바뀜\n",
    "        \n",
    "        return None\n",
    "    \n",
    "class Embedding:\n",
    "    \n",
    "    def __init__(self, W):\n",
    "        \n",
    "        self.params = [W] # (V, D)\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.W = W\n",
    "        self.layers = []\n",
    "        \n",
    "    def forward(self, input_x):\n",
    "        # input:  input_x  (N, T)\n",
    "        # output: x        (N, T, D)\n",
    "        \n",
    "        N, T = input_x.shape\n",
    "        V, D = self.W.shape\n",
    "        \n",
    "        x = np.empty((N, T, D), dtype='f')\n",
    "        for t in range(T):\n",
    "            layer = EmbeddingLayer(self.W)\n",
    "            x[:, t, :] = layer.forward(input_x[:, t]) # (N, D)\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def backward(self, dx):\n",
    "        # input dx: (N, T, D)\n",
    "        \n",
    "        N, T, D = dx.shape\n",
    "        \n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dx[:, t, :])\n",
    "            grad += layer.grads[0]    # (V, D)\n",
    "        \n",
    "        self.grads[0][...] = grad\n",
    "         \n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124eaec",
   "metadata": {},
   "source": [
    "#### Rnn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b980a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLayer:\n",
    "    \n",
    "    def __init__(self, Wh, Wx, b):\n",
    "        \n",
    "        self.params = [Wh, Wx, b]\n",
    "        self.grads = [np.zeros_like(Wh), np.zeros_like(Wx), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        # input:  x (N, D), h_prev (N, H)\n",
    "        # output: h_next (N, H)\n",
    "        \n",
    "        Wh, Wx, b = self.params\n",
    "        t = np.matmul(h_prev, Wh) + np.matmul(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "        \n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        \n",
    "        return h_next\n",
    "        \n",
    "    def backward(self, dh_next):\n",
    "        # input:  dh_next (N, H)\n",
    "        # output: dx (N, D) dh_prev (N, H)\n",
    "        \n",
    "        Wh, Wx, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "        \n",
    "        dt = dh_next * (1 - h_next**2) # (N, H)\n",
    "        db = np.sum(dt, axis=0)        # (H, )\n",
    "        \n",
    "        dWh = np.matmul(h_prev.T, dt)  # (H, N) (N, H)\n",
    "        dh_prev = np.matmul(dt, Wh)    # (N, H) (H, H)\n",
    "        \n",
    "        dWx = np.matmul(x.T, dt)       # (D, N) (N, H)\n",
    "        dx = np.matmul(dt, Wx.T)       # (N, H) (H, D)\n",
    "        \n",
    "        self.grads[0][...] = dWh\n",
    "        self.grads[1][...] = dWx\n",
    "        self.grads[2][...] = db\n",
    "        \n",
    "        return dx, dh_prev\n",
    "    \n",
    "class Rnn:\n",
    "    \n",
    "    def __init__(self, Wh, Wx, b, stateful=False):\n",
    "        \n",
    "        self.params = [Wh, Wx, b]\n",
    "        self.grads = [np.zeros_like(Wh), np.zeros_like(Wx), np.zeros_like(b)]     \n",
    "        \n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "        self.layers = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input:  x (N, T, D)\n",
    "        # output: h (N, T, H)\n",
    "        \n",
    "        Wh, Wx, b = self.params\n",
    "        N, T, D = x.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        h = np.empty((N, T, H), dtype='f')\n",
    "        if self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = RnnLayer(Wh, Wx, b)\n",
    "            self.h = layer.forward(x[:, t, :], self.h)\n",
    "            h[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        return h\n",
    "        \n",
    "    def backward(self, dh):\n",
    "        # input:  dh (N, T, H)\n",
    "        # output: dx (N, T, D)\n",
    "        \n",
    "        Wh, Wx, b = self.params\n",
    "        N, T, H = dh.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        dx = np.empty((N, T, D), dtype='f')\n",
    "        grads = [0, 0, 0]\n",
    "        \n",
    "        _dh = 0\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            _dx, _dh = layer.backward(dh[:, t, :] + _dh)\n",
    "            dx[:, t, :] = _dx\n",
    "            \n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "        \n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "            \n",
    "        self.dh = _dh\n",
    "        \n",
    "        return dx\n",
    "        \n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b93911",
   "metadata": {},
   "source": [
    "#### Affine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e67eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    \n",
    "    def __init__(self, Wa, ba):\n",
    "        \n",
    "        self.params = [Wa, ba]\n",
    "        self.grads = [np.zeros_like(Wa), np.zeros_like(ba)]\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, h):\n",
    "        # input:  h (N, T, H)\n",
    "        # output: a (N, T, V)\n",
    "        \n",
    "        Wa, ba = self.params\n",
    "        N, T, H = h.shape\n",
    "        \n",
    "        h = h.reshape(N*T, -1)     # (NT, H)\n",
    "        a = np.matmul(h, Wa) + ba  # (NT, H) (H, V)\n",
    "        \n",
    "        h = h.reshape(N, T, -1)    # (N, T, H)\n",
    "        a = a.reshape(N, T, -1)    # (N, T, V)\n",
    "        \n",
    "        self.cache = h\n",
    "        \n",
    "        return a\n",
    "        \n",
    "    def backward(self, da):\n",
    "        # input:  da (N, T, V)\n",
    "        # output: dh (N, T, H)\n",
    "        \n",
    "        Wa, ba = self.params\n",
    "        N, T, V = da.shape\n",
    "        \n",
    "        h = self.cache             # (N, T, H)\n",
    "        h = h.reshape(N*T, -1)     # (NT, H)\n",
    "        da = da.reshape(N*T, -1)   # (NT, V)\n",
    "    \n",
    "        dWa = np.matmul(h.T, da)   # (H, NT) (NT, V)\n",
    "        dba = np.sum(da, axis=0)   # (V, )\n",
    "        dh = np.matmul(da, Wa.T)   # (NT, V) (V, H)\n",
    "        dh = dh.reshape(N, T, -1)  # (N, T, H)\n",
    "        \n",
    "        self.grads[0][...] = dWa\n",
    "        self.grads[1][...] = dba\n",
    "        \n",
    "        return dh\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d64d82",
   "metadata": {},
   "source": [
    "#### softmax class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "109f5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, a, label):\n",
    "        # input: a (N, T, V), label (N, T)\n",
    "        # output y (N, T)\n",
    "        \n",
    "        N, T, V = a.shape\n",
    "        _a = a.reshape(N*T, -1)                   # (NT, V)\n",
    "        _label = label.reshape(N*T)               # (NT, )\n",
    "        \n",
    "        _a = _a - _a.max(axis=1, keepdims=True)   # (NT, V)\n",
    "        ea = np.exp(_a)                           # (NT, V)\n",
    "        \n",
    "        _y = ea / ea.sum(axis=1, keepdims=True)   # (NT, V)\n",
    "        _y = _y[np.arange(N*T), _label]           # (NT, )\n",
    "        y = _y.reshape(N, T)\n",
    "        \n",
    "        self.cache = (a, y, label)\n",
    "    \n",
    "        return y\n",
    "        \n",
    "    def backward(self, dy):\n",
    "        # input: dy (N, T)\n",
    "        # output da (N, T, V)\n",
    "        \n",
    "        N, T = dy.shape\n",
    "        _dy = dy.reshape(N*T)         # (NT, )\n",
    "        \n",
    "        a, y, label = self.cache\n",
    "        _a = a.reshape(N*T, -1)       # (NT, V)\n",
    "        _y = y.reshape(N*T)           # (NT, )\n",
    "        _label = label.reshape(N*T)   # (NT, )\n",
    "        \n",
    "        _a[np.arange(N*T), _label] = _dy * (_y * (1 - _y))\n",
    "        \n",
    "        da = _a.reshape(N, T, -1)\n",
    "        \n",
    "        return da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8edf44c",
   "metadata": {},
   "source": [
    "#### CeLoss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a914f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CeLoss:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, y):\n",
    "        # input: y (N, T)\n",
    "        # output: loss (1, 1)\n",
    "        \n",
    "        N, T = y.shape\n",
    "        _y = y.reshape(N*T)\n",
    "        l = -np.log(_y)\n",
    "        loss = np.sum(l) / N*T\n",
    "        \n",
    "        self.cache = y\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def backward(self, dloss=1):\n",
    "        # input: dloss (1, 1)\n",
    "        # output: dy (N, T)\n",
    "        \n",
    "        y = self.cache\n",
    "        N, T = y.shape\n",
    "        \n",
    "        _y = y.reshape(N*T)   # (NT, )\n",
    "        dy = dloss * (-1/_y)  # (NT, ) \n",
    "        \n",
    "        return dy.reshape(N, T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa0eb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLM:\n",
    "    \n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.rand\n",
    "        \n",
    "        # 파라미터 초기화 \n",
    "        W = (rn(V, D) / np.sqrt(V)).astype('f')\n",
    "        \n",
    "        Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
    "        Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
    "        b = np.zeros(H).astype('f')\n",
    "        \n",
    "        Wa = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        ba = np.zeros(V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [Embedding(W),\n",
    "                       Rnn(Wh, Wx, b, stateful=True),\n",
    "                       Affine(Wa, ba)]\n",
    "        self.softmax = Softmax()\n",
    "        self.loss = CeLoss()\n",
    "        \n",
    "        # 파라미터, 기울기 \n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params \n",
    "            self.grads += layer.grads \n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # input x (N, T), t (N, T)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        y = self.softmax.forward(x, t) \n",
    "        loss = self.loss.forward(y)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        dys = self.loss.backward()        # (N, T)\n",
    "        dout = self.softmax.backward(dys) # (N, T, V)\n",
    "    \n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9428516e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba4187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
