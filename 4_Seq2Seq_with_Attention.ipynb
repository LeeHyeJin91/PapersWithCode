{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428f4b67",
   "metadata": {},
   "source": [
    "### Attention 구현\n",
    "\n",
    "* Attention()\n",
    "    * AttentionLayer()\n",
    "        * AttentionWeight()\n",
    "        * WeightSum()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582bd38",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/LeeHyeJin91/PapersWithCode/assets/43728746/d8ba99ae-c5a4-4a22-acc9-97df3e6d0ea2\" width=800 hight=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df76cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc0a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeight:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = SoftmaxLayer()\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        # input: hs(N, T, H), h(N, H)\n",
    "        # output: a(N, T)\n",
    "        \n",
    "        N, T, H = hs.shape\n",
    "        h = h.reshape(N, 1, H).repeat(T, axis=1) # (N, T, H)\n",
    "        \n",
    "        t = hs * h                               # (N, T, H)\n",
    "        s = np.sum(t, axis=2)                    # (N, T)\n",
    "        a = self.softmax.forward(s)              # (N, T)\n",
    "        \n",
    "        self.cache = (hs, h)\n",
    "        \n",
    "        return a\n",
    "        \n",
    "    def backward(self, da):\n",
    "        # input: da(N, T)\n",
    "        # output: dhs(N, T, H), dh(N, H)\n",
    "        \n",
    "        hs, h = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ds = self.softmax.backward(da)            # (N, T)\n",
    "        dt = s.reshape(N, T, 1).repeat(H, axis=2) # (N, T, H)\n",
    "        dhs = dt * h                              # (N, T, H)\n",
    "        dh = dt * hs                              # (N, T, H)\n",
    "        dh = np.sum(dh, axis=1)                   # (N, H)\n",
    "        \n",
    "        return dhs, dh\n",
    "\n",
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = self.out * dout\n",
    "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
    "        dx -= self.out * sumdx\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc7419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, a):\n",
    "        # input: hs(N, T, H), a(N, T)\n",
    "        # output: c(N, H)\n",
    "        \n",
    "        N, T, H = hs.shape\n",
    "        a = a.reshape(N, T, 1).repeat(H, axis=2)  # (N, T, H)\n",
    "        t = hs * a                                # (N, T, H)\n",
    "        c = np.sum(t, axis=1)                     # (N, T, H)\n",
    "        \n",
    "        self.cache = (hs, a)\n",
    "        \n",
    "        return c\n",
    "        \n",
    "    def backward(self, dc):\n",
    "        # input: dc(N, H)\n",
    "        # output: dhs(N, T, H), da(N, T)\n",
    "        \n",
    "        hs, a = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)  # (N, T, H)\n",
    "        dhs = dt * a                                # (N, T, H)\n",
    "        da = dt * hs                                # (N, T, H)\n",
    "        da = np.sum(da, axis=2)                     # (N, T) \n",
    "    \n",
    "        return dhs, da\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24ac0b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        # input: hs(N, T, H), h(N, H)\n",
    "        # output: c(N, H)\n",
    "        \n",
    "        a = self.attention_weight_layer.forward(hs, h) # (N, T)\n",
    "        c = self.weight_sum_layer.forward(hs, a)       # (N, H)\n",
    "        self.attention_weight = a                      # (N, T)\n",
    "        \n",
    "        return c\n",
    "        \n",
    "    def backward(self, dc):\n",
    "        # input: dc(N, H)\n",
    "        # output: dhs(N, T, H), dh(N, H)\n",
    "        \n",
    "        dhs0, da = self.weight_sum.backward(dc)       # (N, T, H) (N, T)\n",
    "        dhs1, dh = self.attention_weight.forward(da)  # (N, T, H) (N, H) \n",
    "        dhs = dhs0+dhs1                               # (N, T, H) \n",
    "        \n",
    "        return dhs, dh\n",
    "    \n",
    "class Attention:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = []\n",
    "        self.attention_weight = []\n",
    "        \n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        # input: hs_enc(N, T1, H), hs_dec(N, T2, H)\n",
    "        # output: cs(N, T2, H)\n",
    "        \n",
    "        N, T2, H = hs_dec.shape\n",
    "        cs = np.empty((N, T2, H), dtype='f')\n",
    "        \n",
    "        for t in range(T2):\n",
    "            layer = AttentionLayer()\n",
    "            c = layer.forward(hs_enc, hs_dec[:, t, :])           # (N, H)\n",
    "            cs[:, t, :] = c\n",
    "            self.attention_weight.append(layer.attention_weight) # (T2, N, T1)\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        return cs\n",
    "        \n",
    "    def backward(self, dcs):\n",
    "        # input: dcs(N, T2, H)  \n",
    "        # output: dhs_enc(N, T1, H), dhs_dec(N, T2, H)\n",
    "        \n",
    "        N, T2, H = dcs.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty((N, T2, H), dtype='f')\n",
    "        \n",
    "        for t in range(T2):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dcs[:, t, :]) # (N, T1, H) (N, H)\n",
    "            \n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:, t, :] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4002a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5793ca7",
   "metadata": {},
   "source": [
    "### Seq2Seq 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe18ad24",
   "metadata": {},
   "source": [
    "     \n",
    "* **AttentionSeq2Seq()**\n",
    "    * Encoder()\n",
    "        * Embedding()\n",
    "        * LSTM()\n",
    "    * Decoder()\n",
    "        * Embedding()\n",
    "        * LSTM() \n",
    "        * Attention()\n",
    "        * Affine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1994593b",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/LeeHyeJin91/PapersWithCode/assets/43728746/62502786-4bed-4366-8ce3-f4a36a286760\" width=900 hight=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f65efd8",
   "metadata": {},
   "source": [
    "#### decoder 상세구조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4fd4f",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/LeeHyeJin91/PapersWithCode/assets/43728746/62b4f081-85b3-4935-a070-7527fd1ac1cc\" width=500 hight=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e8b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from LSTM import Embedding, LSTM, Affine, Softmax, CEloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e7dbe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    \n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        \n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.rand\n",
    "        \n",
    "        W = (rn(V, D)/100).astype('f')\n",
    "        Wx = (rn(D, 4*H) / np.sqrt(D)).astype('f')\n",
    "        Wh = (rn(H, 4*H) / np.sqrt(H)).astype('f')\n",
    "        b = np.zeros(4*H).astype('f')\n",
    "        \n",
    "        self.embd = Embedding(W)\n",
    "        self.lstm = LSTM(Wx, Wh, b, stateful=False)\n",
    "        \n",
    "        self.params = self.embd.params + self.lstm.params\n",
    "        self.grads = self.embd.grads + self.lstm.grads\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input: x   (N, T1)\n",
    "        # output: hs (N, T1, H)\n",
    "        \n",
    "        xs = self.embd.forward(x) # (N, T1, D)\n",
    "        hs = self.lstm.forward(x) # (N, T1, H)\n",
    "    \n",
    "        return hs\n",
    "        \n",
    "    def backward(self, dhs):\n",
    "        # input: dhs (N, T1, H)\n",
    "        # output: dout\n",
    "    \n",
    "        dxs = self.lstm.backward(dhs)  # (N, T1, H)\n",
    "        dout = self.embd.backward(dxs) # (N, T1, D) 임베딩 업데이트 \n",
    "        \n",
    "        return dout\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16648f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    \n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        \n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.rand\n",
    "        \n",
    "        W = (rn(V, D)/100).astype('f')\n",
    "        Wx = (rn(D, 4*H) / np.sqrt(D)).astype('f')\n",
    "        Wh = (rn(H, 4*H) / np.sqrt(H)).astype('f')\n",
    "        b = np.zeros(4*H).astype('f')\n",
    "        Wa = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        ba = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.embd = Embedding(W)\n",
    "        self.lstm = LSTM(Wx, Wh, b, stateful=True)\n",
    "        self.attention = Attention()\n",
    "        self.affine = Affine(Wa, ba)\n",
    "        \n",
    "        layers = [self.embd, self.lstm, self.attention, self.affine]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in [self.embd, self.lstm, self.attention, self.affine]:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "     \n",
    "    def forward(self, t, hs_enc):\n",
    "        # input: t(N, T2), hs_enc(N, T1, H)\n",
    "        # output: a(N, T2, V)\n",
    "        \n",
    "        h = hs_enc[:, -1, :]\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        xs = self.embd.forward(t)                   # (N, T2, D)\n",
    "        hs_dec = self.lstm.forward(xs)              # (N, T2, H)\n",
    "        _cs = self.attention(hs_enc, hs_dec)        # (N, T2, H)\n",
    "        \n",
    "        cs = np.concatenate((_cs, hs_dec), axis=2)  # (N, T2, 2H)\n",
    "        a = self.affine.forward(cs)                 # (N, T2, V)\n",
    "        \n",
    "        return a\n",
    "        \n",
    "    def backward(self, da):\n",
    "        # input: da       (N, T2, V)\n",
    "        # output: dhs_enc (N, T1, H)\n",
    "        \n",
    "        dcs = self.affine.backward(da)                   # (N, T2, 2H)\n",
    "        N, T2, H2 = dcs.sahpe\n",
    "        H = H2//2\n",
    "        dcs0, dcs1 = dcs[:, :, :H], dcs[:, :, H:]        # (N, T2, H) (N, T2, H) \n",
    "        \n",
    "        dhs_enc, dhs_dec = self.attention.backward(dcs0) # (N, T1, H) (N, T2, H) \n",
    "        dhs = dhs_dec + dcs1                             # (N, T2, H)\n",
    "        dxs = self.lstm.backward(dhs)                    # (N, T2, D)\n",
    "        dout = self.embd.backward(dxs)                   # 임베딩 업데이트\n",
    "        \n",
    "        # output\n",
    "        dh = self.lstm.dh                                # (N, H)\n",
    "        dhs_enc[:, -1] += dh                             # (N, T1, H)\n",
    "    \n",
    "        return dhs_enc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c18e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq:\n",
    "    \n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        \n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = Decoder(V, D, H)\n",
    "        self.softmax = Softmax()\n",
    "        self.loss = Celoss()\n",
    "        \n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # input: x(N, T1), t(N, T2)\n",
    "        # output:loss(1, 1)\n",
    "        \n",
    "        decoder_x = t[:, :-1]\n",
    "        decoder_t = t[:, 1:]\n",
    "        \n",
    "        hs_enc = self.encoder.forward(x)            # (N, T1, H)\n",
    "        a = self.decoder.forward(decoder_x, hs_enc) # (N, T2, V)\n",
    "        y = self.softmax.forward(a, decoder_t)      # (N, T2)\n",
    "        loss = self.loss.forward(y)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dloss=1):\n",
    "         \n",
    "        dy = self.loss.backward(dloss)        # (N, T2)\n",
    "        da = self.softmax.backward(dy)        # (N, T2, V)\n",
    "        dhs_enc = self.decoder.backward(da)   # (N, T1, H)\n",
    "        dout = self.encoder.backward(dhs_enc) # (N, T1, D)\n",
    "        \n",
    "        return dout\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd9d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
